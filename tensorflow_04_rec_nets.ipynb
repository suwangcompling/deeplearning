{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn.ptb import reader\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. LSTM LANGUAGE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CODE FROM TUTORIAL (BIG DATA UNIVERSITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-01-01 15:59:10--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
      "Resolving www.fit.vutbr.cz... 2001:67c:1220:809::93e5:917, 147.229.9.23\n",
      "Connecting to www.fit.vutbr.cz|2001:67c:1220:809::93e5:917|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34869662 (33M) [application/x-gtar]\n",
      "Saving to: ‘simple-examples.tgz’\n",
      "\n",
      "simple-examples.tgz 100%[===================>]  33.25M   777KB/s    in 53s     \n",
      "\n",
      "2017-01-01 16:00:04 (640 KB/s) - ‘simple-examples.tgz’ saved [34869662/34869662]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA (PENN TREEBANK)\n",
    "\n",
    "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz \n",
    "!tar xzf simple-examples.tgz -C /Users/jacobsw/Desktop/CODER/TENSORFLOW/DATA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SET HYPERPARAMETERS\n",
    "\n",
    "#Initial weight scale\n",
    "init_scale = 0.1\n",
    "#Initial learning rate\n",
    "learning_rate = 1.0\n",
    "#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)\n",
    "max_grad_norm = 5\n",
    "#The number of layers in our model\n",
    "num_layers = 2\n",
    "#The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\n",
    "num_steps = 20\n",
    "#The number of processing units (neurons) in the hidden layers\n",
    "hidden_size = 200\n",
    "#The maximum number of epochs trained with the initial learning rate\n",
    "max_epoch = 4\n",
    "#The total number of epochs in training\n",
    "max_max_epoch = 13\n",
    "#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)\n",
    "#At 1, we ignore the Dropout Layer wrapping.\n",
    "keep_prob = 1\n",
    "#The decay for the learning rate\n",
    "decay = 0.5\n",
    "#The size for each batch of data\n",
    "batch_size = 20\n",
    "#The size of our vocabulary\n",
    "vocab_size = 10000\n",
    "#Training flag to separate training from testing\n",
    "is_training = 1\n",
    "#Data directory for our dataset\n",
    "data_dir = \"/Users/jacobsw/Desktop/CODER/TENSORFLOW/DATA/simple-examples/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "\n",
    "    def __init__(self, is_training):\n",
    "        ######################################\n",
    "        # Setting parameters for ease of use #\n",
    "        ######################################\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        ###############################################################################\n",
    "        # Creating placeholders for our input data and expected outputs (target data) #\n",
    "        ###############################################################################\n",
    "        self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        self._targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "\n",
    "        ##########################################################################\n",
    "        # Creating the LSTM cell structure and connect it with the RNN structure #\n",
    "        ##########################################################################\n",
    "        # Create the LSTM unit. Size is the same as the size of our hidden layer, and no bias is added to the Forget Gate\n",
    "        # This creates only the structure for the LSTM and has to be associated with a RNN unit still\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(size, forget_bias=0.0, state_is_tuple=False)\n",
    "        \n",
    "        # Unless you changed keep_prob, this won't actually execute -- this is a dropout wrapper for our LSTM unit\n",
    "        # This is an optimization of the LSTM output, but is not needed at all\n",
    "        if is_training and keep_prob < 1:\n",
    "            lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n",
    "        \n",
    "        # By taking in the LSTM cells as parameters, the MultiRNNCell function junctions the LSTM units to the RNN units\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * num_layers, state_is_tuple=False)\n",
    "        \n",
    "        # Define the initial state, i.e., the model state for the very first data point\n",
    "        self._initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        ####################################################################\n",
    "        # Creating the word embeddings and pointing them to the input data #\n",
    "        ####################################################################\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            # Create the embeddings for our input data\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, size])\n",
    "            # Define where to get the data for our embeddings from\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n",
    "\n",
    "        # Unless you changed keep_prob, this won't actually execute -- this is a dropout addition for our inputs\n",
    "        # This is an optimization of the input processing and is not needed at all\n",
    "        if is_training and keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, keep_prob)\n",
    "\n",
    "        ############################################\n",
    "        # Creating the input structure for our RNN #\n",
    "        ############################################\n",
    "        # The input structure is fed from the embeddings, which are filled in by the input data\n",
    "        inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, num_steps, inputs)]\n",
    "        \n",
    "        ####################################################################################################\n",
    "        # Instanciating our RNN model and retrieving the structure for returning the outputs and the state #\n",
    "        ####################################################################################################\n",
    "        outputs, state = rnn.rnn(cell, inputs, initial_state=self._initial_state)\n",
    "\n",
    "        #########################################################################\n",
    "        # Creating a logistic unit to return the probability of the output word #\n",
    "        #########################################################################\n",
    "        output = tf.reshape(tf.concat(1, outputs), [-1, size])\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [size, vocab_size])\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "\n",
    "        #########################################################################\n",
    "        # Defining the loss and cost functions for the model's learning to work #\n",
    "        #########################################################################\n",
    "        loss = tf.nn.seq2seq.sequence_loss_by_example([logits], [tf.reshape(self._targets, [-1])],\n",
    "                                                      [tf.ones([batch_size * num_steps])])\n",
    "        self._cost = cost = tf.reduce_sum(loss) / batch_size\n",
    "\n",
    "        # Store the final state\n",
    "        self._final_state = state\n",
    "\n",
    "        #Everything after this point is relevant only for training\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        #################################################\n",
    "        # Creating the Training Operation for our Model #\n",
    "        #################################################\n",
    "        # Create a variable for the learning rate\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "        tvars = tf.trainable_variables()\n",
    "        # Define the gradient clipping threshold\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), max_grad_norm)\n",
    "        # Create the gradient descent optimizer with our learning rate\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        # Create the training TensorFlow Operation through our optimizer\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    # Helper functions for our LSTM RNN class\n",
    "\n",
    "    # Assign the learning rate for this model\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "    # Returns the input data for this model at a point in time\n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "\n",
    "    # Returns the targets for this model at a point in time\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    # Returns the initial state for this model\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    # Returns the defined Cost\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    # Returns the final state for this model\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    # Returns the current learning rate for this model\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    # Returns the training operation defined for this model\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OLD reader.ptb_iterator is deprecated and removed.\n",
    "\n",
    "def ptb_iterator(raw_data, batch_size, num_steps):\n",
    "    raw_data = np.array(raw_data, dtype=np.int32)\n",
    "\n",
    "    data_len = len(raw_data)\n",
    "    batch_len = data_len // batch_size\n",
    "    data = np.zeros([batch_size, batch_len], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n",
    "\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "    if epoch_size == 0:\n",
    "        raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        x = data[:, i*num_steps:(i+1)*num_steps]\n",
    "        y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
    "        yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "# run_epoch takes as parameters the current session, the model instance, the data to be fed, and the operation to be run #\n",
    "##########################################################################################################################\n",
    "def run_epoch(session, m, data, eval_op, verbose=False):\n",
    "\n",
    "    #Define the epoch size based on the length of the data, batch size and the number of steps\n",
    "    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    state = m.initial_state.eval()\n",
    "    \n",
    "    #For each step and data point\n",
    "#     for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size,\n",
    "#                                                     m.num_steps)):\n",
    "    for step, (x, y) in enumerate(ptb_iterator(data, m.batch_size,\n",
    "                                                    m.num_steps)):\n",
    "        \n",
    "        #Evaluate and return cost, state by running cost, final_state and the function passed as parameter\n",
    "        cost, state, _ = session.run([m.cost, m.final_state, eval_op],\n",
    "                                     {m.input_data: x,\n",
    "                                      m.targets: y,\n",
    "                                      m.initial_state: state})\n",
    "        \n",
    "        #Add returned cost to costs (which keeps track of the total costs for this epoch)\n",
    "        costs += cost\n",
    "        \n",
    "        #Add number of steps to iteration counter\n",
    "        iters += m.num_steps\n",
    "\n",
    "        if verbose and step % (epoch_size // 10) == 10:\n",
    "            print(\"%.3f perplexity: %.3f speed: %.0f wps\" % (step * 1.0 / epoch_size, np.exp(costs / iters),\n",
    "              iters * m.batch_size / (time.time() - start_time)))\n",
    "\n",
    "    # Returns the Perplexity rating for us to keep track of how the model is evolving\n",
    "    return np.exp(costs / iters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x11b01b890>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x10ff7dd50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x10ff46490>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "Epoch 1 : Learning rate: 1.000\n",
      "0.004 perplexity: 5725.722 speed: 1326 wps\n",
      "0.104 perplexity: 855.605 speed: 1755 wps\n",
      "0.204 perplexity: 633.067 speed: 1732 wps\n",
      "0.304 perplexity: 511.406 speed: 1770 wps\n",
      "0.404 perplexity: 440.481 speed: 1797 wps\n",
      "0.504 perplexity: 394.260 speed: 1789 wps\n",
      "0.604 perplexity: 354.727 speed: 1782 wps\n",
      "0.703 perplexity: 327.548 speed: 1791 wps\n",
      "0.803 perplexity: 306.208 speed: 1800 wps\n",
      "0.903 perplexity: 286.498 speed: 1810 wps\n",
      "Epoch 1 : Train Perplexity: 271.746\n",
      "Epoch 1 : Valid Perplexity: 181.218\n",
      "Epoch 2 : Learning rate: 1.000\n",
      "0.004 perplexity: 214.239 speed: 1816 wps\n",
      "0.104 perplexity: 151.318 speed: 1848 wps\n",
      "0.204 perplexity: 158.296 speed: 1786 wps\n",
      "0.304 perplexity: 153.225 speed: 1751 wps\n",
      "0.404 perplexity: 150.338 speed: 1766 wps\n",
      "0.504 perplexity: 147.750 speed: 1765 wps\n",
      "0.604 perplexity: 143.100 speed: 1766 wps\n",
      "0.703 perplexity: 140.890 speed: 1778 wps\n",
      "0.803 perplexity: 138.810 speed: 1789 wps\n",
      "0.903 perplexity: 135.050 speed: 1797 wps\n",
      "Epoch 2 : Train Perplexity: 132.879\n",
      "Epoch 2 : Valid Perplexity: 142.747\n",
      "Epoch 3 : Learning rate: 1.000\n",
      "0.004 perplexity: 144.620 speed: 1898 wps\n",
      "0.104 perplexity: 103.682 speed: 1863 wps\n",
      "0.204 perplexity: 112.647 speed: 1866 wps\n",
      "0.304 perplexity: 109.788 speed: 1866 wps\n",
      "0.404 perplexity: 108.809 speed: 1853 wps\n",
      "0.504 perplexity: 107.904 speed: 1845 wps\n",
      "0.604 perplexity: 105.295 speed: 1813 wps\n",
      "0.703 perplexity: 104.657 speed: 1788 wps\n",
      "0.803 perplexity: 104.002 speed: 1766 wps\n",
      "0.903 perplexity: 101.750 speed: 1750 wps\n",
      "Epoch 3 : Train Perplexity: 100.752\n",
      "Epoch 3 : Valid Perplexity: 130.811\n",
      "Epoch 4 : Learning rate: 1.000\n",
      "0.004 perplexity: 113.782 speed: 1493 wps\n",
      "0.104 perplexity: 83.530 speed: 1554 wps\n",
      "0.204 perplexity: 91.795 speed: 1553 wps\n",
      "0.304 perplexity: 89.637 speed: 1555 wps\n",
      "0.404 perplexity: 89.129 speed: 1553 wps\n",
      "0.504 perplexity: 88.712 speed: 1551 wps\n",
      "0.604 perplexity: 86.929 speed: 1552 wps\n",
      "0.703 perplexity: 86.789 speed: 1552 wps\n",
      "0.803 perplexity: 86.580 speed: 1556 wps\n",
      "0.903 perplexity: 84.946 speed: 1559 wps\n",
      "Epoch 4 : Train Perplexity: 84.426\n",
      "Epoch 4 : Valid Perplexity: 126.629\n",
      "Epoch 5 : Learning rate: 1.000\n",
      "0.004 perplexity: 96.666 speed: 1589 wps\n",
      "0.104 perplexity: 72.298 speed: 1568 wps\n",
      "0.204 perplexity: 79.773 speed: 1560 wps\n",
      "0.304 perplexity: 78.086 speed: 1555 wps\n",
      "0.404 perplexity: 77.856 speed: 1548 wps\n",
      "0.504 perplexity: 77.618 speed: 1541 wps\n",
      "0.604 perplexity: 76.210 speed: 1527 wps\n",
      "0.703 perplexity: 76.231 speed: 1522 wps\n",
      "0.803 perplexity: 76.188 speed: 1533 wps\n",
      "0.903 perplexity: 74.909 speed: 1559 wps\n",
      "Epoch 5 : Train Perplexity: 74.553\n",
      "Epoch 5 : Valid Perplexity: 126.165\n",
      "Epoch 6 : Learning rate: 0.500\n",
      "0.004 perplexity: 86.122 speed: 1875 wps\n",
      "0.104 perplexity: 63.170 speed: 1802 wps\n",
      "0.204 perplexity: 68.423 speed: 1804 wps\n",
      "0.304 perplexity: 65.994 speed: 1740 wps\n",
      "0.404 perplexity: 64.931 speed: 1662 wps\n",
      "0.504 perplexity: 64.030 speed: 1618 wps\n",
      "0.604 perplexity: 62.141 speed: 1590 wps\n",
      "0.703 perplexity: 61.473 speed: 1566 wps\n",
      "0.803 perplexity: 60.758 speed: 1557 wps\n",
      "0.903 perplexity: 59.055 speed: 1551 wps\n",
      "Epoch 6 : Train Perplexity: 58.182\n",
      "Epoch 6 : Valid Perplexity: 119.518\n",
      "Epoch 7 : Learning rate: 0.250\n",
      "0.004 perplexity: 71.006 speed: 1523 wps\n",
      "0.104 perplexity: 52.226 speed: 1494 wps\n",
      "0.204 perplexity: 56.723 speed: 1499 wps\n",
      "0.304 perplexity: 54.635 speed: 1508 wps\n",
      "0.404 perplexity: 53.736 speed: 1510 wps\n",
      "0.504 perplexity: 52.877 speed: 1513 wps\n",
      "0.604 perplexity: 51.214 speed: 1522 wps\n",
      "0.703 perplexity: 50.574 speed: 1526 wps\n",
      "0.803 perplexity: 49.828 speed: 1544 wps\n",
      "0.903 perplexity: 48.260 speed: 1553 wps\n",
      "Epoch 7 : Train Perplexity: 47.398\n",
      "Epoch 7 : Valid Perplexity: 120.023\n",
      "Epoch 8 : Learning rate: 0.125\n",
      "0.004 perplexity: 63.542 speed: 1730 wps\n",
      "0.104 perplexity: 46.336 speed: 1736 wps\n",
      "0.204 perplexity: 50.392 speed: 1739 wps\n",
      "0.304 perplexity: 48.455 speed: 1740 wps\n",
      "0.404 perplexity: 47.643 speed: 1740 wps\n",
      "0.504 perplexity: 46.844 speed: 1742 wps\n",
      "0.604 perplexity: 45.345 speed: 1744 wps\n",
      "0.703 perplexity: 44.747 speed: 1742 wps\n",
      "0.803 perplexity: 44.045 speed: 1741 wps\n",
      "0.903 perplexity: 42.586 speed: 1742 wps\n",
      "Epoch 8 : Train Perplexity: 41.765\n",
      "Epoch 8 : Valid Perplexity: 121.251\n",
      "Epoch 9 : Learning rate: 0.062\n",
      "0.004 perplexity: 59.861 speed: 1747 wps\n",
      "0.104 perplexity: 43.352 speed: 1740 wps\n",
      "0.204 perplexity: 47.182 speed: 1737 wps\n",
      "0.304 perplexity: 45.339 speed: 1738 wps\n",
      "0.404 perplexity: 44.563 speed: 1735 wps\n",
      "0.504 perplexity: 43.809 speed: 1736 wps\n",
      "0.604 perplexity: 42.403 speed: 1737 wps\n",
      "0.703 perplexity: 41.819 speed: 1736 wps\n",
      "0.803 perplexity: 41.141 speed: 1734 wps\n",
      "0.903 perplexity: 39.744 speed: 1734 wps\n",
      "Epoch 9 : Train Perplexity: 38.943\n",
      "Epoch 9 : Valid Perplexity: 122.098\n",
      "Epoch 10 : Learning rate: 0.031\n",
      "0.004 perplexity: 57.911 speed: 1730 wps\n",
      "0.104 perplexity: 41.856 speed: 1722 wps\n",
      "0.204 perplexity: 45.546 speed: 1726 wps\n",
      "0.304 perplexity: 43.739 speed: 1720 wps\n",
      "0.404 perplexity: 42.984 speed: 1751 wps\n",
      "0.504 perplexity: 42.252 speed: 1757 wps\n",
      "0.604 perplexity: 40.895 speed: 1745 wps\n",
      "0.703 perplexity: 40.314 speed: 1723 wps\n",
      "0.803 perplexity: 39.643 speed: 1704 wps\n",
      "0.903 perplexity: 38.278 speed: 1687 wps\n",
      "Epoch 10 : Train Perplexity: 37.486\n",
      "Epoch 10 : Valid Perplexity: 122.256\n",
      "Epoch 11 : Learning rate: 0.016\n",
      "0.004 perplexity: 56.752 speed: 1598 wps\n",
      "0.104 perplexity: 41.049 speed: 1570 wps\n",
      "0.204 perplexity: 44.674 speed: 1566 wps\n",
      "0.304 perplexity: 42.881 speed: 1562 wps\n",
      "0.404 perplexity: 42.141 speed: 1559 wps\n",
      "0.504 perplexity: 41.422 speed: 1559 wps\n",
      "0.604 perplexity: 40.086 speed: 1558 wps\n",
      "0.703 perplexity: 39.509 speed: 1558 wps\n",
      "0.803 perplexity: 38.839 speed: 1557 wps\n",
      "0.903 perplexity: 37.492 speed: 1568 wps\n",
      "Epoch 11 : Train Perplexity: 36.703\n",
      "Epoch 11 : Valid Perplexity: 121.958\n",
      "Epoch 12 : Learning rate: 0.008\n",
      "0.004 perplexity: 56.000 speed: 1798 wps\n",
      "0.104 perplexity: 40.539 speed: 1779 wps\n",
      "0.204 perplexity: 44.153 speed: 1777 wps\n",
      "0.304 perplexity: 42.381 speed: 1791 wps\n",
      "0.404 perplexity: 41.657 speed: 1796 wps\n",
      "0.504 perplexity: 40.949 speed: 1794 wps\n",
      "0.604 perplexity: 39.624 speed: 1793 wps\n",
      "0.703 perplexity: 39.050 speed: 1792 wps\n",
      "0.803 perplexity: 38.381 speed: 1791 wps\n",
      "0.903 perplexity: 37.047 speed: 1791 wps\n",
      "Epoch 12 : Train Perplexity: 36.262\n",
      "Epoch 12 : Valid Perplexity: 121.555\n",
      "Epoch 13 : Learning rate: 0.004\n",
      "0.004 perplexity: 55.559 speed: 1800 wps\n",
      "0.104 perplexity: 40.221 speed: 1785 wps\n",
      "0.204 perplexity: 43.830 speed: 1789 wps\n",
      "0.304 perplexity: 42.080 speed: 1792 wps\n",
      "0.404 perplexity: 41.372 speed: 1792 wps\n",
      "0.504 perplexity: 40.671 speed: 1789 wps\n",
      "0.604 perplexity: 39.358 speed: 1790 wps\n",
      "0.703 perplexity: 38.786 speed: 1791 wps\n",
      "0.803 perplexity: 38.121 speed: 1791 wps\n",
      "0.903 perplexity: 36.794 speed: 1791 wps\n",
      "Epoch 13 : Train Perplexity: 36.013\n",
      "Epoch 13 : Valid Perplexity: 121.288\n",
      "Test Perplexity: 115.979\n"
     ]
    }
   ],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, _ = raw_data\n",
    "\n",
    "#Initializes the Execution Graph and the Session\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    initializer = tf.random_uniform_initializer(-init_scale,\n",
    "                                            init_scale)\n",
    "    \n",
    "    # Instantiates the model for training\n",
    "    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "        m = PTBModel(is_training=True)\n",
    "        \n",
    "    # Reuses the trained parameters for the validation and testing models\n",
    "    # They are different instances but use the same variables for weights and biases, they just don't change when data is input\n",
    "    with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "        mvalid = PTBModel(is_training=False)\n",
    "        mtest = PTBModel(is_training=False)\n",
    "\n",
    "    #Initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for i in range(max_max_epoch):\n",
    "        # Define the decay for this epoch\n",
    "        lr_decay = decay ** max(i - max_epoch, 0.0)\n",
    "        \n",
    "        # Set the decayed learning rate as the learning rate for this epoch\n",
    "        m.assign_lr(session, learning_rate * lr_decay)\n",
    "\n",
    "        print(\"Epoch %d : Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "        \n",
    "        # Run the loop for this epoch in the training model\n",
    "        train_perplexity = run_epoch(session, m, train_data, m.train_op,\n",
    "                                   verbose=True)\n",
    "        print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "        \n",
    "        # Run the loop for this epoch in the validation model\n",
    "        valid_perplexity = run_epoch(session, mvalid, valid_data, tf.no_op())\n",
    "        print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "    \n",
    "    # Run the loop in the testing model to see how effective was our training\n",
    "    test_perplexity = run_epoch(session, mtest, test_data, tf.no_op())\n",
    "    \n",
    "    print(\"Test Perplexity: %.3f\" % test_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. LSTM SEQUENCE CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('.', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Sizes:  (55000, 784) (55000, 10)\n",
      "Test Sizes:  (10000, 784) (10000, 10)\n",
      "Sample:\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAEUCAYAAAALPosBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12589ce50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280, Minibatch Loss = 1.876079, Training Accuracy = 0.34375\n",
      "Iter 2560, Minibatch Loss = 1.412820, Training Accuracy = 0.50000\n",
      "Iter 3840, Minibatch Loss = 1.217277, Training Accuracy = 0.57031\n",
      "Iter 5120, Minibatch Loss = 0.979868, Training Accuracy = 0.65625\n",
      "Iter 6400, Minibatch Loss = 0.665566, Training Accuracy = 0.78906\n",
      "Iter 7680, Minibatch Loss = 0.615495, Training Accuracy = 0.80469\n",
      "Iter 8960, Minibatch Loss = 0.462367, Training Accuracy = 0.88281\n",
      "Iter 10240, Minibatch Loss = 0.389916, Training Accuracy = 0.86719\n",
      "Iter 11520, Minibatch Loss = 0.237537, Training Accuracy = 0.91406\n",
      "Iter 12800, Minibatch Loss = 0.366488, Training Accuracy = 0.86719\n",
      "Iter 14080, Minibatch Loss = 0.418528, Training Accuracy = 0.88281\n",
      "Iter 15360, Minibatch Loss = 0.283164, Training Accuracy = 0.91406\n",
      "Iter 16640, Minibatch Loss = 0.215019, Training Accuracy = 0.93750\n",
      "Iter 17920, Minibatch Loss = 0.296394, Training Accuracy = 0.88281\n",
      "Iter 19200, Minibatch Loss = 0.239417, Training Accuracy = 0.91406\n",
      "Iter 20480, Minibatch Loss = 0.231106, Training Accuracy = 0.92188\n",
      "Iter 21760, Minibatch Loss = 0.310261, Training Accuracy = 0.89844\n",
      "Iter 23040, Minibatch Loss = 0.263579, Training Accuracy = 0.93750\n",
      "Iter 24320, Minibatch Loss = 0.203553, Training Accuracy = 0.91406\n",
      "Iter 25600, Minibatch Loss = 0.237684, Training Accuracy = 0.94531\n",
      "Iter 26880, Minibatch Loss = 0.208338, Training Accuracy = 0.91406\n",
      "Iter 28160, Minibatch Loss = 0.269308, Training Accuracy = 0.91406\n",
      "Iter 29440, Minibatch Loss = 0.252970, Training Accuracy = 0.92969\n",
      "Iter 30720, Minibatch Loss = 0.225922, Training Accuracy = 0.95312\n",
      "Iter 32000, Minibatch Loss = 0.146690, Training Accuracy = 0.93750\n",
      "Iter 33280, Minibatch Loss = 0.129667, Training Accuracy = 0.96094\n",
      "Iter 34560, Minibatch Loss = 0.126684, Training Accuracy = 0.96875\n",
      "Iter 35840, Minibatch Loss = 0.093026, Training Accuracy = 0.96875\n",
      "Iter 37120, Minibatch Loss = 0.246609, Training Accuracy = 0.90625\n",
      "Iter 38400, Minibatch Loss = 0.135407, Training Accuracy = 0.93750\n",
      "Iter 39680, Minibatch Loss = 0.172159, Training Accuracy = 0.93750\n",
      "Iter 40960, Minibatch Loss = 0.135378, Training Accuracy = 0.96875\n",
      "Iter 42240, Minibatch Loss = 0.145011, Training Accuracy = 0.96875\n",
      "Iter 43520, Minibatch Loss = 0.195220, Training Accuracy = 0.92969\n",
      "Iter 44800, Minibatch Loss = 0.192450, Training Accuracy = 0.93750\n",
      "Iter 46080, Minibatch Loss = 0.181640, Training Accuracy = 0.96094\n",
      "Iter 47360, Minibatch Loss = 0.192879, Training Accuracy = 0.95312\n",
      "Iter 48640, Minibatch Loss = 0.108915, Training Accuracy = 0.97656\n",
      "Iter 49920, Minibatch Loss = 0.139645, Training Accuracy = 0.94531\n",
      "Iter 51200, Minibatch Loss = 0.172660, Training Accuracy = 0.92969\n",
      "Iter 52480, Minibatch Loss = 0.180858, Training Accuracy = 0.96094\n",
      "Iter 53760, Minibatch Loss = 0.084937, Training Accuracy = 0.96875\n",
      "Iter 55040, Minibatch Loss = 0.131635, Training Accuracy = 0.96094\n",
      "Iter 56320, Minibatch Loss = 0.095086, Training Accuracy = 0.97656\n",
      "Iter 57600, Minibatch Loss = 0.127391, Training Accuracy = 0.96875\n",
      "Iter 58880, Minibatch Loss = 0.219067, Training Accuracy = 0.92188\n",
      "Iter 60160, Minibatch Loss = 0.110942, Training Accuracy = 0.94531\n",
      "Iter 61440, Minibatch Loss = 0.089440, Training Accuracy = 0.96875\n",
      "Iter 62720, Minibatch Loss = 0.196931, Training Accuracy = 0.94531\n",
      "Iter 64000, Minibatch Loss = 0.081592, Training Accuracy = 0.98438\n",
      "Iter 65280, Minibatch Loss = 0.073859, Training Accuracy = 0.96094\n",
      "Iter 66560, Minibatch Loss = 0.117047, Training Accuracy = 0.98438\n",
      "Iter 67840, Minibatch Loss = 0.113109, Training Accuracy = 0.96094\n",
      "Iter 69120, Minibatch Loss = 0.087674, Training Accuracy = 0.97656\n",
      "Iter 70400, Minibatch Loss = 0.105661, Training Accuracy = 0.96094\n",
      "Iter 71680, Minibatch Loss = 0.164087, Training Accuracy = 0.95312\n",
      "Iter 72960, Minibatch Loss = 0.173806, Training Accuracy = 0.96094\n",
      "Iter 74240, Minibatch Loss = 0.036266, Training Accuracy = 0.99219\n",
      "Iter 75520, Minibatch Loss = 0.093014, Training Accuracy = 0.97656\n",
      "Iter 76800, Minibatch Loss = 0.069247, Training Accuracy = 0.99219\n",
      "Iter 78080, Minibatch Loss = 0.078437, Training Accuracy = 0.98438\n",
      "Iter 79360, Minibatch Loss = 0.127861, Training Accuracy = 0.97656\n",
      "Iter 80640, Minibatch Loss = 0.143995, Training Accuracy = 0.95312\n",
      "Iter 81920, Minibatch Loss = 0.054932, Training Accuracy = 0.98438\n",
      "Iter 83200, Minibatch Loss = 0.092052, Training Accuracy = 0.96094\n",
      "Iter 84480, Minibatch Loss = 0.075809, Training Accuracy = 0.96875\n",
      "Iter 85760, Minibatch Loss = 0.142536, Training Accuracy = 0.95312\n",
      "Iter 87040, Minibatch Loss = 0.118662, Training Accuracy = 0.98438\n",
      "Iter 88320, Minibatch Loss = 0.089704, Training Accuracy = 0.97656\n",
      "Iter 89600, Minibatch Loss = 0.086510, Training Accuracy = 0.98438\n",
      "Iter 90880, Minibatch Loss = 0.086119, Training Accuracy = 0.97656\n",
      "Iter 92160, Minibatch Loss = 0.051071, Training Accuracy = 0.98438\n",
      "Iter 93440, Minibatch Loss = 0.040320, Training Accuracy = 0.98438\n",
      "Iter 94720, Minibatch Loss = 0.079792, Training Accuracy = 0.96875\n",
      "Iter 96000, Minibatch Loss = 0.143674, Training Accuracy = 0.97656\n",
      "Iter 97280, Minibatch Loss = 0.056106, Training Accuracy = 0.98438\n",
      "Iter 98560, Minibatch Loss = 0.165993, Training Accuracy = 0.94531\n",
      "Iter 99840, Minibatch Loss = 0.066716, Training Accuracy = 0.98438\n",
      "Optimization Finished\n",
      "Testing Accuracy:  0.9668\n"
     ]
    }
   ],
   "source": [
    "# DATA INFO\n",
    "\n",
    "trainX, trainY, testX, testY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n",
    "print('Train Sizes: ', trainX.shape, trainY.shape)\n",
    "print('Test Sizes: ', testX.shape, testY.shape)\n",
    "print\n",
    "\n",
    "print(\"Sample:\\n\")\n",
    "plt.imshow(trainX[0].reshape([28,28]), cmap='gray')\n",
    "plt.xlabel(np.argmax(trainY[0]), fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "# PREPARE PARAMETERS\n",
    "\n",
    "n_input = 28 # width\n",
    "n_steps = 28 # height\n",
    "n_hidden = 128\n",
    "n_classes = 10\n",
    "learning_rate = .001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# BUILD GRAPH\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, n_steps, n_input], name='x')\n",
    "y = tf.placeholder(tf.float32, shape=[None, n_classes], name='y')\n",
    "weights = tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "biases = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "def LSTM(x, weights, biases): # x = [batch_size, n_steps, n_input].\n",
    "    x = tf.transpose(x, [1,0,2])\n",
    "    x = tf.reshape(x, [-1,n_input]) # reshape to (n_steps*batch_size, n_input)\n",
    "    x = tf.split(split_dim=0, num_split=n_steps, value=x) # get a list of n_steps tensors of shape (batch_size, n_input).\n",
    "    lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.)\n",
    "    outputs, states = rnn.rnn(cell=lstm_cell, inputs=x, dtype=tf.float32)\n",
    "    return tf.matmul(outputs[-1], weights) + biases # only take output from the last step (not a sequence labeling task).\n",
    "with tf.variable_scope('fwd'):\n",
    "    pred = LSTM(x, weights, biases)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "correct = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# TRAINING\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    step = 1\n",
    "    while step*batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        sess.run(optimizer, feed_dict={x:batch_x, y:batch_y})\n",
    "        if step%display_step == 0:\n",
    "            cur_acc, cur_loss = sess.run([accuracy, loss], feed_dict={x:batch_x, y:batch_y})\n",
    "            print('Iter '+str(step*batch_size) + ', Minibatch Loss = ' + \\\n",
    "                  '{:.6f}'.format(cur_loss) + ', Training Accuracy = ' + \\\n",
    "                  '{:.5f}'.format(cur_acc))\n",
    "        step += 1\n",
    "    print('Optimization Finished')\n",
    "\n",
    "# EVALUATION\n",
    "\n",
    "#     test_len = 128\n",
    "#     test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "#     test_label = mnist.test.labels[:test_len]\n",
    "    test_data = testX.reshape((-1, n_steps, n_input))\n",
    "    test_label = testY\n",
    "    print('Testing Accuracy: ', \\\n",
    "          sess.run(accuracy, feed_dict={x:test_data, y:test_label}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. LSTM SEQUENCE LABELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ATIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
