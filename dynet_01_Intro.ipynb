{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DyNet Intro\n",
    "\n",
    "* **I. API**\n",
    "* **II. E.G.1 -- XOR**\n",
    "* **III. E.G.2 -- RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dynet import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "[1.0, 2.0, 3.0]\n",
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]]\n",
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]]\n"
     ]
    }
   ],
   "source": [
    "# Reset comp-graph\n",
    "#   always a good practice before starting a session\n",
    "renew_cg()\n",
    "\n",
    "# Create data structures\n",
    "x = scalarInput(2)\n",
    "v = inputVector([1,2,3]) \n",
    "z = inputTensor(np.array([[1,2,3],[4,5,6]]))\n",
    "print x.value()\n",
    "print v.vec_value()\n",
    "print z.value()\n",
    "print z.npvalue() # eval as np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 5: [0.12060943990945816, 0.6338164210319519, 0.41185712814331055]\n",
      "row 5 (non-trainable): [0.12060943990945816, 0.6338164210319519, 0.41185712814331055]\n",
      "Reset to row 9 ...\n",
      "row 5: [-0.5574085116386414, 0.38705575466156006, 0.7486212253570557]\n",
      "row 5 (non-trainable): [-0.5574085116386414, 0.38705575466156006, 0.7486212253570557]\n"
     ]
    }
   ],
   "source": [
    "# Filling params and embedding lookup\n",
    "\n",
    "renew_cg()\n",
    "\n",
    "m = ParameterCollection()\n",
    "pW = m.add_parameters((8,8))\n",
    "pb = m.add_parameters(8)\n",
    "\n",
    "W = parameter(pW)\n",
    "b = parameter(pb)\n",
    "\n",
    "vocab_size = 10\n",
    "emb_size = 3\n",
    "lp = m.add_lookup_parameters((vocab_size, emb_size))\n",
    "\n",
    "e5 = lookup(lp, 5) # get embedding at row 5\n",
    "e5c = lookup(lp, 5, update=False) # set as non-trainable\n",
    "print 'row 5:', e5.value()\n",
    "print 'row 5 (non-trainable):', e5c.value()\n",
    "e5.set(9) # e5 expr. now has row 9\n",
    "e5c.set(9) # same\n",
    "print 'Reset to row 9 ...'\n",
    "print 'row 5:', e5.value()\n",
    "print 'row 5 (non-trainable):', e5c.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementary ops\n",
      "5.0\n",
      "26.0\n",
      "[3.0, 8.0, 15.0]\n",
      "[0.3333333432674408, 0.5, 0.6000000238418579]\n",
      "6.0\n",
      "\n",
      "Reshape & Transpose\n",
      "[[ 1.  3.  5.]\n",
      " [ 2.  4.  6.]]\n",
      "[[ 1.  2.]\n",
      " [ 3.  4.]\n",
      " [ 5.  6.]]\n",
      "\n",
      "Activations\n",
      "[0.7615941762924194, 0.9640275835990906, 0.9950547218322754]\n",
      "[2.7182817459106445, 7.389056205749512, 20.08553695678711]\n",
      "[0.0, 0.6931471824645996, 1.0986123085021973]\n",
      "[0.7310585975646973, 0.8807970285415649, 0.9525741338729858]\n",
      "[1.0, 2.0, 3.0]\n",
      "[0.5, 0.6666666865348816, 0.75]\n",
      "[0.09003058075904846, 0.24472849071025848, 0.665241003036499]\n",
      "\n",
      "Restricted ops to ids\n",
      "[-2.1269280910491943, -inf, -0.12692809104919434]\n",
      "\n",
      "Select vals by ids\n",
      "3.0\n",
      "3.0\n",
      "[3.0, 4.0, 5.0]\n",
      "[3.0, 4.0, 5.0]\n",
      "6.45862960815\n",
      "\n",
      "Regularization\n",
      "[0.9812455773353577, 2.005760669708252, 2.999218702316284]\n",
      "[2.0, 0.0, 6.0]\n",
      "\n",
      "For-all ops\n",
      "[4.0, 6.0, 8.0]\n",
      "[2.0, 3.0, 4.0]\n",
      "[[ 1.  3.]\n",
      " [ 2.  4.]\n",
      " [ 3.  5.]]\n",
      "[1.0, 2.0, 3.0, 3.0, 4.0, 5.0]\n",
      "\n",
      "Loss fns\n",
      "12.0\n",
      "6.0\n",
      "10.7129249573\n",
      "0.2217797786\n",
      "2.0\n",
      "[[ 3.  3.  3.]]\n"
     ]
    }
   ],
   "source": [
    "# Basic operations\n",
    "\n",
    "renew_cg() # CRUCIAL! OTHERWISE DEAD KERNEL\n",
    "\n",
    "a_scalar = scalarInput(2)\n",
    "b_scalar = scalarInput(3)\n",
    "a_vec = inputVector([1,2,3])\n",
    "b_vec = inputVector([3,4,5])\n",
    "\n",
    "print 'Elementary ops'\n",
    "print (a_scalar + b_scalar).value()\n",
    "print dot_product(a_vec, b_vec).value()\n",
    "print cmult(a_vec, b_vec).value() # Hadamard product\n",
    "print cdiv(a_vec, b_vec).value() # Hadamard division (CRUCIAL: __future__.division required)\n",
    "print sum_elems(a_vec).value()\n",
    "# print colwise_add(a_vec, b_vec)  \n",
    "print\n",
    "\n",
    "print 'Reshape & Transpose'\n",
    "c_vec = inputVector([1,2,3,4,5,6])\n",
    "c_mat = reshape(c_vec, (2,3))\n",
    "print c_mat.value()\n",
    "print transpose(c_mat).value()\n",
    "print \n",
    "\n",
    "print 'Activations'\n",
    "print tanh(a_vec).value()\n",
    "print exp(a_vec).value()\n",
    "print log(a_vec).value()\n",
    "print logistic(a_vec).value()\n",
    "print rectify(a_vec).value()\n",
    "print softsign(a_vec).value()\n",
    "print softmax(a_vec).value()\n",
    "print \n",
    "\n",
    "print 'Restricted ops to ids'\n",
    "print log_softmax(a_vec,restrict=[0,2]).value()\n",
    "print \n",
    "\n",
    "print 'Select vals by ids'\n",
    "d_vec = inputVector(range(10))\n",
    "print pick(d_vec, 3).value()\n",
    "print d_vec[3].value()\n",
    "print pickrange(d_vec, 3, 6).value()\n",
    "print d_vec[3:6].value()\n",
    "print pickneglogsoftmax(d_vec, 3).value() #  i.e. (pick(-log(softmax(e1)), k))\n",
    "print \n",
    "\n",
    "print 'Regularization'\n",
    "print noise(a_vec, 0.01).value() # add Gaussian noise with std=0.01\n",
    "print dropout(a_vec, 0.5).value() # dropout with p=0.5, dropped vals=0\n",
    "print \n",
    "\n",
    "print 'For-all ops'\n",
    "print esum([a_vec, b_vec]).value() # Hadamard sum\n",
    "print average([a_vec, b_vec]).value() # Hadamard avg.\n",
    "print concatenate_cols([a_vec, b_vec]).value() # concat vecs as cols\n",
    "print concatenate([a_vec, b_vec]).value() # long-list concat\n",
    "print \n",
    "\n",
    "print 'Loss fns'\n",
    "print squared_distance(a_vec, b_vec).value() # Euclidean dist.\n",
    "print l1_distance(a_vec, b_vec).value() # Manhattan dist.\n",
    "print huber_distance(a_vec, b_vec).value() # less sensitive to outliers like Euclidean\n",
    "print binary_log_loss(logistic(a_scalar), logistic(b_scalar)).value() # b*log(a) + (1-b)*log(1-a)\n",
    "print pairwise_rank_loss(a_scalar, b_scalar, m=1.0).value() \n",
    "print pairwise_rank_loss(transpose(a_vec), transpose(b_vec), m=1.0).value()\n",
    "\n",
    "# For conv net support, better use Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loss at 500: 24.1510372162\n",
      "... loss at 1000: 24.6248283386\n",
      "... loss at 1500: 23.3079299927\n",
      "... loss at 2000: 25.1814155579\n",
      "... loss at 2500: 23.0396842957\n",
      "... loss at 3000: 20.3630332947\n",
      "... loss at 3500: 23.7845077515\n",
      "... loss at 4000: 23.2671813965\n",
      "... loss at 4500: 22.9774723053\n",
      "... loss at 5000: 23.629901886\n",
      "... loss at 5500: 22.718000412\n"
     ]
    }
   ],
   "source": [
    "# Simple MNIST logistic classifier\n",
    "#   warning: slow, trying to figure out why it's so much slower than TF\n",
    "\n",
    "from dynet import *\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "print \"Train sizes\", mnist.train.images.shape, mnist.train.labels.shape\n",
    "print \"Test sizes\", mnist.test.images.shape, mnist.test.labels.shape\n",
    "\n",
    "dp = DynetParams()\n",
    "dp.set_autobatch(True)\n",
    "dp.set_mem(2048)\n",
    "\n",
    "renew_cg()\n",
    "\n",
    "m = ParameterCollection()\n",
    "pW = m.add_parameters((10, 784))\n",
    "pb = m.add_parameters(10)\n",
    "\n",
    "trainer = AdamTrainer(m)\n",
    "\n",
    "W = parameter(pW)\n",
    "b = parameter(pb)\n",
    "x = inputVector(np.zeros(784))\n",
    "def calculate_loss(X_i, y_i):\n",
    "    x.set(X_i)\n",
    "    pred = softmax((W*x)+b)\n",
    "    loss = -log(pick(pred, np.argmax(y_i)))\n",
    "    return loss\n",
    "count = 0\n",
    "num_epochs = 1\n",
    "batch_size = 10\n",
    "for _ in xrange(num_epochs):\n",
    "    for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "        count += 1\n",
    "        X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        losses = []\n",
    "        for X_i,y_i in zip(X_batch,y_batch):\n",
    "            loss = calculate_loss(X_i,y_i)\n",
    "            losses.append(loss)\n",
    "        loss = esum(losses)\n",
    "        if count % 500 == 0:\n",
    "            print '... loss at', str(count)+':', loss.value()\n",
    "        loss.forward()\n",
    "        loss.backward()\n",
    "        trainer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4882\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "results = []\n",
    "for X_i,y_i in zip(mnist.test.images,mnist.test.labels):\n",
    "    x.set(X_i)\n",
    "    pred = np.argmax(softmax((W*x)+b).value())\n",
    "    true = np.argmax(y_i)\n",
    "    results.append(1 if pred==true else 0)\n",
    "print sum(results)/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "... loss at 500: 0.339906\n",
      "... loss at 1000: 0.380015\n",
      "... loss at 1500: 0.242546\n",
      "... loss at 2000: 0.298855\n",
      "... loss at 2500: 0.346771\n",
      "... loss at 3000: 0.14689\n",
      "... loss at 3500: 0.346184\n",
      "... loss at 4000: 0.37455\n",
      "... loss at 4500: 0.0738878\n",
      "... loss at 5000: 0.77124\n",
      "... loss at 5500: 0.340216\n",
      "\n",
      "Test\n",
      "Accuracy: 0.9199\n"
     ]
    }
   ],
   "source": [
    "# Simple TF MNIST logistic classifier\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "print \"Train sizes\", mnist.train.images.shape, mnist.train.labels.shape\n",
    "print \"Test sizes\", mnist.test.images.shape, mnist.test.labels.shape\n",
    "print\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess: sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "import random\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X_ph = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ph = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W = tf.Variable(tf.truncated_normal(shape=[784, 10], stddev=0.01))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "pred = tf.nn.softmax(tf.matmul(X_ph, W) + b)\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_ph*tf.log(pred), reduction_indices=1))\n",
    "\n",
    "trainer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 10\n",
    "count = 0\n",
    "print \"Train\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for _ in xrange(num_epochs):\n",
    "        for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "            count += 1\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            _, loss_ = sess.run([trainer, loss], feed_dict={X_ph:X_batch,y_ph:y_batch})\n",
    "            if count % 500 == 0:\n",
    "                print '... loss at', str(count)+':', loss_\n",
    "    \n",
    "    print \n",
    "    print \"Test\"\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y_ph, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    print \"Accuracy:\", accuracy.eval({X_ph:mnist.test.images, y_ph:mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. XOR\n",
    "\n",
    "* $\\sigma(V(\\texttt{tanh}(Wx+b)))$\n",
    "* $W\\in\\mathbb{R}^{8\\times2},V\\in\\mathbb{R}^{1\\times8},b\\in\\mathbb{R}^8$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. STATIC NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dynet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create param collection and add params\n",
    "m = ParameterCollection()\n",
    "pW = m.add_parameters((8,2)) # print pW.shape() to get shape\n",
    "pV = m.add_parameters((1,8))\n",
    "pb = m.add_parameters((8))\n",
    "\n",
    "# Reset comp-graph\n",
    "renew_cg()\n",
    "\n",
    "# Associate params with comp-graph\n",
    "#   type -> cg Expressions\n",
    "\n",
    "W = parameter(pW)\n",
    "V = parameter(pV)\n",
    "b = parameter(pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.1485757678747177,\n",
       " 0.08643510937690735,\n",
       " -0.21179868280887604,\n",
       " 0.3174358010292053,\n",
       " 0.008616679348051548,\n",
       " -0.8477156162261963,\n",
       " 0.12413163483142853,\n",
       " 0.7179639339447021]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.371465533972\n",
      "0.361458718777\n",
      "1.19289886951\n"
     ]
    }
   ],
   "source": [
    "# Build main comp-graph\n",
    "x = vecInput(2) # alternatively, inputVector([...])\n",
    "output = logistic(V*(tanh((W*x)+b)))\n",
    "y = scalarInput(0) # gold\n",
    "loss = binary_log_loss(output, y)\n",
    "\n",
    "# E.G. compute output on randomly set values\n",
    "x.set([0,0])\n",
    "print output.value()\n",
    "\n",
    "# E.G. compute loss on randomly set values\n",
    "x.set([1,0])\n",
    "y.set(0) # set the wrong answer\n",
    "print loss.value() # expect more loss when trained\n",
    "y.set(1) # set the correct answer\n",
    "print loss.value() # expect less loss when trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss before step is: 1.19289886951\n",
      "the loss after step is: 1.027146101\n"
     ]
    }
   ],
   "source": [
    "# Add trainer\n",
    "trainer = SimpleSGDTrainer(m)\n",
    "\n",
    "# E.G. test trainer node\n",
    "#   the loss before training should be higher\n",
    "x.set([1,0])\n",
    "y.set(1)\n",
    "loss_value = loss.value()\n",
    "print \"the loss before step is:\", loss_value\n",
    "loss.backward() # compute grads\n",
    "trainer.update() # train\n",
    "loss_value = loss.value(recalculate=True)\n",
    "print \"the loss after step is:\", loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) 0\n",
      "(0, 1) 1\n",
      "(1, 0) 1\n",
      "(1, 1) 0\n",
      "\n",
      "Test results\n",
      "0,1 0.579762995243\n",
      "1,0 0.358027279377\n",
      "0,0 0.405996859074\n",
      "1,1 0.474987447262\n"
     ]
    }
   ],
   "source": [
    "# Create training data\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "vals = [0,1]\n",
    "def create_xor_instances(num_rounds=2000):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for _ in xrange(num_rounds):\n",
    "        for x1,x2 in product(vals, vals):\n",
    "            answer = 0 if x1==x2 else 1\n",
    "            questions.append((x1,x2))\n",
    "            answers.append(answer)\n",
    "    return questions, answers\n",
    "\n",
    "X_train, y_train = create_xor_instances()\n",
    "for i in range(4):\n",
    "    print X_train[i], y_train[i]\n",
    "print\n",
    "    \n",
    "# Model tester\n",
    "\n",
    "def test_xor():\n",
    "    print \"Test results\"\n",
    "    x.set([0,1])\n",
    "    print \"0,1\", output.value()\n",
    "    x.set([1,0])\n",
    "    print \"1,0\", output.value()\n",
    "    x.set([0,0])\n",
    "    print \"0,0\", output.value()\n",
    "    x.set([1,1])\n",
    "    print \"1,1\", output.value()\n",
    "    \n",
    "test_xor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss is: 0.720788167119\n",
      "average loss is: 0.697766235471\n",
      "average loss is: 0.655933563411\n",
      "average loss is: 0.593452329263\n",
      "average loss is: 0.523161799997\n",
      "average loss is: 0.459621861863\n",
      "average loss is: 0.407008892074\n",
      "average loss is: 0.364187619663\n",
      "average loss is: 0.32912860419\n",
      "average loss is: 0.300072345642\n",
      "average loss is: 0.275672255118\n",
      "average loss is: 0.254925653837\n",
      "average loss is: 0.237085357768\n",
      "average loss is: 0.22158883813\n",
      "average loss is: 0.208006923498\n",
      "average loss is: 0.196007620273\n",
      "average loss is: 0.185330641564\n",
      "average loss is: 0.175769331319\n",
      "average loss is: 0.167157687647\n",
      "average loss is: 0.15936092629\n",
      "average loss is: 0.152268522817\n",
      "average loss is: 0.145789020409\n",
      "average loss is: 0.139846107338\n",
      "average loss is: 0.134375619021\n",
      "average loss is: 0.129323218989\n",
      "average loss is: 0.124642592538\n",
      "average loss is: 0.120294022751\n",
      "average loss is: 0.116243255748\n",
      "average loss is: 0.112460595337\n",
      "average loss is: 0.10892016964\n",
      "average loss is: 0.105599334471\n",
      "average loss is: 0.102478186194\n",
      "average loss is: 0.0995391594631\n",
      "average loss is: 0.0967666926427\n",
      "average loss is: 0.0941469509584\n",
      "average loss is: 0.0916675926125\n",
      "average loss is: 0.0893175737909\n",
      "average loss is: 0.0870869814389\n",
      "average loss is: 0.0849668918036\n",
      "average loss is: 0.0829492505483\n",
      "average loss is: 0.0810267694164\n",
      "average loss is: 0.0791928366415\n",
      "average loss is: 0.0774414405264\n",
      "average loss is: 0.0757671024835\n",
      "average loss is: 0.0741648195894\n",
      "average loss is: 0.0726300142293\n",
      "average loss is: 0.0711584887467\n",
      "average loss is: 0.0697463877601\n",
      "average loss is: 0.0683901633766\n",
      "average loss is: 0.0670865456511\n",
      "average loss is: 0.0658325152349\n",
      "average loss is: 0.064625280211\n",
      "average loss is: 0.0634622549986\n",
      "average loss is: 0.0623410414672\n",
      "average loss is: 0.0612594121495\n",
      "average loss is: 0.060215295606\n",
      "average loss is: 0.0592067626804\n",
      "average loss is: 0.0582320149104\n",
      "average loss is: 0.0572893729646\n",
      "average loss is: 0.0563772676531\n",
      "average loss is: 0.0554942304588\n",
      "average loss is: 0.0546388857546\n",
      "average loss is: 0.0538099437436\n",
      "average loss is: 0.053006193617\n",
      "average loss is: 0.052226497845\n",
      "average loss is: 0.0514697862885\n",
      "average loss is: 0.0507350517656\n",
      "average loss is: 0.0500213450596\n",
      "average loss is: 0.0493277712815\n",
      "average loss is: 0.0486534853264\n",
      "average loss is: 0.0479976893484\n",
      "average loss is: 0.0473596287437\n",
      "average loss is: 0.0467385895102\n",
      "average loss is: 0.0461338956936\n",
      "average loss is: 0.0455449066872\n",
      "average loss is: 0.044971015151\n",
      "average loss is: 0.044411644575\n",
      "average loss is: 0.0438662476243\n",
      "average loss is: 0.043334304178\n",
      "average loss is: 0.0428153197253\n",
      "\n",
      "Test results\n",
      "0,1 0.998068153858\n",
      "1,0 0.998003423214\n",
      "0,0 0.00134725833777\n",
      "1,1 0.00191594380885\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "total_loss = 0\n",
    "seen_instances = 0\n",
    "avg_losses = []\n",
    "for X_i, y_i in zip(X_train,y_train):\n",
    "    x.set(X_i)\n",
    "    y.set(y_i)\n",
    "    seen_instances += 1\n",
    "    total_loss += loss.value()\n",
    "    loss.backward()\n",
    "    trainer.update()\n",
    "    if (seen_instances > 1) and seen_instances % 100 == 0:\n",
    "        avg_loss = total_loss / seen_instances\n",
    "        avg_losses.append(avg_loss)\n",
    "        print \"average loss is:\", avg_loss\n",
    "        \n",
    "print\n",
    "test_xor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUhARkX7/H/ekDXNZ2YtrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f51021c21d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(avg_losses)\n",
    "plt.title(\"Static XOR\")\n",
    "plt.xlabel(\"#100iters\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. DYNAMIC NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dynet import *\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss is: 0.752134788036\n",
      "average loss is: 0.737397335172\n",
      "average loss is: 0.725410375396\n",
      "average loss is: 0.704652427137\n",
      "average loss is: 0.660070686638\n",
      "average loss is: 0.597163186744\n",
      "average loss is: 0.535364472408\n",
      "average loss is: 0.481906592813\n",
      "average loss is: 0.436995918875\n",
      "average loss is: 0.39928848009\n",
      "average loss is: 0.367386082101\n",
      "average loss is: 0.340131206056\n",
      "average loss is: 0.316617591948\n",
      "average loss is: 0.296144262494\n",
      "average loss is: 0.278167582033\n",
      "average loss is: 0.262262479412\n",
      "average loss is: 0.248093210172\n",
      "average loss is: 0.235391770889\n",
      "average loss is: 0.223942008297\n",
      "average loss is: 0.213567864006\n",
      "average loss is: 0.204124606276\n",
      "average loss is: 0.195492229276\n",
      "average loss is: 0.187570429088\n",
      "average loss is: 0.180274746849\n",
      "average loss is: 0.173533572807\n",
      "average loss is: 0.167285804526\n",
      "average loss is: 0.161478992104\n",
      "average loss is: 0.156067864394\n",
      "average loss is: 0.151013143858\n",
      "average loss is: 0.14628058757\n",
      "average loss is: 0.141840206198\n",
      "average loss is: 0.137665624854\n",
      "average loss is: 0.133733554861\n",
      "average loss is: 0.130023355025\n",
      "average loss is: 0.126516665449\n",
      "average loss is: 0.123197101285\n",
      "average loss is: 0.120049994342\n",
      "average loss is: 0.11706217316\n",
      "average loss is: 0.11422177733\n",
      "average loss is: 0.111518098335\n",
      "average loss is: 0.108941442218\n",
      "average loss is: 0.106483012873\n",
      "average loss is: 0.104134809858\n",
      "average loss is: 0.101889540394\n",
      "average loss is: 0.0997405429975\n",
      "average loss is: 0.0976817202511\n",
      "average loss is: 0.0957074803634\n",
      "average loss is: 0.0938126858188\n",
      "average loss is: 0.0919926080374\n",
      "average loss is: 0.0902428875116\n",
      "average loss is: 0.0885594982463\n",
      "average loss is: 0.086938716454\n",
      "average loss is: 0.0853770929526\n",
      "average loss is: 0.083871428099\n",
      "average loss is: 0.0824187494668\n",
      "average loss is: 0.0810162921686\n",
      "average loss is: 0.079661480945\n",
      "average loss is: 0.0783519141871\n",
      "average loss is: 0.077085349357\n",
      "average loss is: 0.0758596902794\n",
      "average loss is: 0.0746729752366\n",
      "average loss is: 0.0735233663049\n",
      "average loss is: 0.0724091397644\n",
      "average loss is: 0.0713286774972\n",
      "average loss is: 0.0702804587453\n",
      "average loss is: 0.0692630528647\n",
      "average loss is: 0.0682751131778\n",
      "average loss is: 0.0673153704203\n",
      "average loss is: 0.066382627501\n",
      "average loss is: 0.0654757541196\n",
      "average loss is: 0.0645936826901\n",
      "average loss is: 0.0637354035299\n",
      "average loss is: 0.0628999613422\n",
      "average loss is: 0.0620864514848\n",
      "average loss is: 0.0612940165136\n",
      "average loss is: 0.0605218431577\n",
      "average loss is: 0.0597691597113\n",
      "average loss is: 0.0590352333053\n",
      "average loss is: 0.0583193673182\n",
      "average loss is: 0.0576208995532\n",
      "\n",
      "Test results\n",
      "0,1 0.997336804867\n",
      "1,0 0.997236609459\n",
      "0,0 0.000881387735717\n",
      "1,1 0.00335242063738\n"
     ]
    }
   ],
   "source": [
    "# Create data\n",
    "\n",
    "vals = [0,1]\n",
    "def create_xor_instances(num_rounds=2000):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for _ in xrange(num_rounds):\n",
    "        for x1,x2 in product(vals, vals):\n",
    "            answer = 0 if x1==x2 else 1\n",
    "            questions.append((x1,x2))\n",
    "            answers.append(answer)\n",
    "    return questions, answers\n",
    "\n",
    "X_train, y_train = create_xor_instances()\n",
    "\n",
    "# Create network dynamically\n",
    "\n",
    "def create_xor_network(pW, pV, pb, X_train, y_train):\n",
    "    renew_cg()\n",
    "    W = parameter(pW) \n",
    "    V = parameter(pV)\n",
    "    b = parameter(pb)\n",
    "    x = vecInput(len(X_train))\n",
    "    x.set(X_train)\n",
    "    y = scalarInput(y_train)\n",
    "    output = logistic(V*(tanh((W*x)+b)))\n",
    "    loss =  binary_log_loss(output, y)\n",
    "    return loss\n",
    "\n",
    "# Set params\n",
    "\n",
    "m2 = ParameterCollection()\n",
    "pW = m2.add_parameters((8,2))\n",
    "pV = m2.add_parameters((1,8))\n",
    "pb = m2.add_parameters((8))\n",
    "trainer = SimpleSGDTrainer(m2)\n",
    "\n",
    "# Train\n",
    "\n",
    "seen_instances = 0\n",
    "total_loss = 0\n",
    "avg_losses = []\n",
    "for X_i,y_i in zip(X_train,y_train):\n",
    "    loss = create_xor_network(pW, pV, pb, X_i, y_i)\n",
    "    seen_instances += 1\n",
    "    total_loss += loss.value()\n",
    "    loss.backward()\n",
    "    trainer.update()\n",
    "    if (seen_instances > 1 and seen_instances % 100 == 0):\n",
    "        avg_loss = total_loss / seen_instances\n",
    "        avg_losses.append(avg_loss)\n",
    "        print \"average loss is:\", avg_loss\n",
    "\n",
    "# Test       \n",
    "\n",
    "W = parameter(pW) \n",
    "V = parameter(pV)\n",
    "b = parameter(pb)\n",
    "x = vecInput(2)\n",
    "output = logistic(V*(tanh((W*x)+b)))\n",
    "print\n",
    "print \"Test results\"\n",
    "x.set([0,1])\n",
    "print \"0,1\", output.value()\n",
    "x.set([1,0])\n",
    "print \"1,0\", output.value()\n",
    "x.set([0,0])\n",
    "print \"0,0\", output.value()\n",
    "x.set([1,1])\n",
    "print \"1,1\", output.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCiIiMuD/A7JrXcuDRYwgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f239fc80ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_losses)\n",
    "plt.title(\"Dynamic XOR\")\n",
    "plt.xlabel(\"#100iters\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
